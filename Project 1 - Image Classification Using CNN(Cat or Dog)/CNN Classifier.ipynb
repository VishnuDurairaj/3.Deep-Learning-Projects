{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Dense,Conv2D,Activation,Dropout,ReLU,Flatten,MaxPooling2D,BatchNormalization\n",
    "import cv2\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tutorial we are going to use Microsoft's Cat and Dog Dataset\n",
    "# You can download it from here\n",
    "\n",
    "link = \"https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to convert all the images into arrays\n",
    "# for that we are going to create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the data\n",
    "\n",
    "Location='C:\\path\\to\\your\\PetImages'\n",
    "\n",
    "# in this location i have two folders one is named as Cat and another one is Dog\n",
    "\n",
    "# First we have to loop through both the folders and conver all the images into arrays \n",
    "\n",
    "def prepare_Data(Dir,image_size):\n",
    "    data=[]                                                  # Creating empty list to store data\n",
    "    \n",
    "    categories=os.listdir(Dir)                               # Storing lables(folder names) in a list\n",
    "    \n",
    "    for category in categories:                              # Looping through each and every folder\n",
    "        \n",
    "        path=os.path.join(Dir,category)                      # joining current folder name with the path\n",
    "        \n",
    "        target=categories.index(category)                    # Storing the particular lable as target ['cat','Dog']\n",
    "        \n",
    "        for img in os.listdir(path):                         # Looping through the images \n",
    "            \n",
    "            image=cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE) # Reading the images\n",
    "            try:\n",
    "                image1=cv2.resize(image,(image_size,image_size)) # Images might be in different dimensions so resize it to same dimension\n",
    "                \n",
    "                data.append([image1,target])               # Appending resized image and its lable\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    np.random.shuffle(data)                                # Shuffling the data to avoid [\"cat\",\"cat\",\"cat\",\"dog\",\"dog\",\"gog\"]\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for feature , lable in data:\n",
    "        X.append(feature)                                  # Spliting X,y \n",
    "        y.append(lable)\n",
    "    return X,y,categories\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Call the function to create dataset\n",
    "\n",
    "X,y,data=image2array(Location,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24946, 50, 50)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now our data is ready but it is a list we have to convert it into array\n",
    "X=np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now Shape of X is 3 dim but we need to convert it into 4 dim (-1,50,50,1) [No_sam,height,width,Channels]\n",
    "X=np.array(X).reshape(-1,50,50,1)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each and every time we can not prepare data so it is safe to save our data somewhere\n",
    "\n",
    "# for that i am going to use pickle\n",
    "\n",
    "# Saving the data into pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_in=open('cat_dog_X.pickle','wb')\n",
    "pickle.dump(X,data_in)\n",
    "data_in.close()\n",
    "data_y=open('cat_dog_y.pickle','wb')\n",
    "pickle.dump(y,data_y)\n",
    "data_y.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load our data from pickle\n",
    "\n",
    "# To open data from pickle\n",
    "x=open('cat_dog_X.pickle','rb')\n",
    "X=pickle.load(x)\n",
    "y=open('cat_dog_y.pickle','rb')\n",
    "y=pickle.load(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before feeding into model we have to normalize the data to get better performance\n",
    "\n",
    "# for that we need to first spilt the data\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y)\n",
    "\n",
    "X_train=X_train/255\n",
    "\n",
    "X_test=X_test/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everuthing is ready now let's build the CNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First Convolution layer\n",
    "model.add(Conv2D(filters=60,kernel_size=3,input_shape=X.shape[1:],strides=1,padding='same',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "# Second Convolution layer\n",
    "model.add(Conv2D(filters=60,kernel_size=3,input_shape=X.shape[1:],strides=1,padding='same',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully Connected layer\n",
    "\n",
    "model.add(Dense(80))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Compling the model\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now our Basic CNN model is ready let's train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585/585 [==============================] - 130s 223ms/step - loss: 0.6420 - accuracy: 0.6795 - val_loss: 0.5090 - val_accuracy: 0.7536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x211b98bbc40>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Model works well but is not giving good accuracy. for that we have to tune our model\n",
    "\n",
    "# I am going to use Keras Tuner\n",
    "\n",
    "# Refer this page to know more about kerastune\n",
    "\n",
    "kerastune_DOC = \"https://keras-team.github.io/keras-tuner/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Build a tuner for our model\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "def build_function(hp):  # hp is the hyperparameter\n",
    "    \n",
    "    # first layer will be unique because it consist input_shape so we are defining it outside the loop'\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=hp.Int('inupt_layer',10,100,5),kernel_size=3,input_shape=X_train.shape[1:],strides=1,\n",
    "                     padding='same',kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size =2))\n",
    "    \n",
    "    # Now it's time to decide how many convolution & Dense layer we need to get good performance. \n",
    "    \n",
    "    # for that we are going to use just a for loop\n",
    "    \n",
    "    for i in range(hp.Int(\"conv_layers\",1,5)):       # It will randomly pic no of layer from 1 to 5\n",
    "        \n",
    "    # Here we need to specify the convolution layer without input size\n",
    "        \n",
    "        model.add(Conv2D(filters=hp.Int(f'hidden_conv_neurons{i}_',10,100,5),kernel_size=3,\n",
    "              strides=1,padding='same',kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size =2))\n",
    "    \n",
    "    # after conolution layer we need to flatten the data so let's add one Flatten layer\n",
    "    \n",
    "    model.add(Flatten())  # note it is outside the loop\n",
    "    \n",
    "    # here we have to use one more for loop for Dense layer\n",
    "    for j in range(hp.Int('Dense_layer',1,10)):              # It will randomly pic no of layer from 1 to 10\n",
    "       \n",
    "    # Here we need to specify the Dense layer without input size\n",
    "        \n",
    "        model.add(Dense(units=hp.Int(f\"Dense_{j}_layer\",50,300,20)))\n",
    "        \n",
    "        model.add(Dropout(0.5))             # To avaoid overfitting we are adding dropout it is a regularization technique\n",
    "        \n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    # Finally we need to add output layer\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid')) # This is a binary classification so we are using sigmoid\n",
    "    \n",
    "    # Now we need to compile the model\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now our Tuner is ready let's see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_function,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=1,  \n",
    "    executions_per_trial=1, \n",
    "    directory=LOG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's it we have completed the enire model now let's tune it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "inupt_layer       |40                |?                 \n",
      "conv_layers       |3                 |?                 \n",
      "inupt_0_layer     |70                |?                 \n",
      "Dense_layer       |9                 |?                 \n",
      "Dense_0_layer     |290               |?                 \n",
      "\n",
      "293/293 - 124s - loss: 0.7112 - accuracy: 0.5067 - val_loss: 0.6932 - val_accuracy: 0.5014\n",
      "\n",
      "Trial 1 Complete [00h 02m 07s]\n",
      "val_accuracy: 0.5013628602027893\n",
      "\n",
      "Best val_accuracy So Far: 0.5013628602027893\n",
      "Total elapsed time: 00h 02m 07s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# i am giving only one epoch because i am not using GPU, so it will take toom much time to run.\n",
    "\n",
    "# feel free to increase the epoch if you have GPU setup\n",
    "tuner.search(X_train,y_train,verbose=2,\n",
    "             epochs=1,\n",
    "             batch_size=64,\n",
    "             validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in 1615052476\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "inupt_layer: 40\n",
      "conv_layers: 3\n",
      "inupt_0_layer: 70\n",
      "Dense_layer: 9\n",
      "Dense_0_layer: 290\n",
      "inupt_1_layer: 10\n",
      "inupt_2_layer: 10\n",
      "Dense_1_layer: 50\n",
      "Dense_2_layer: 50\n",
      "Dense_3_layer: 50\n",
      "Dense_4_layer: 50\n",
      "Dense_5_layer: 50\n",
      "Dense_6_layer: 50\n",
      "Dense_7_layer: 50\n",
      "Dense_8_layer: 50\n",
      "Score: 0.5013628602027893\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#After tunig the model we can see the best parameters \n",
    "# Also we can store it \n",
    "print(tuner.results_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training 20 epoch i got 84% val_accuracy if we spend more time on this model we can get even more accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
