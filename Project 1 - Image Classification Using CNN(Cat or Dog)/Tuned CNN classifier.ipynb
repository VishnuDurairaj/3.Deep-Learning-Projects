{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Dense,Conv2D,Activation,Dropout,ReLU,Flatten,MaxPooling2D,BatchNormalization\n",
    "import cv2\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import kerastuner\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the data\n",
    "\n",
    "Location='C:\\path\\to\\your\\PetImages'\n",
    "\n",
    "# in this location i have two folders one is named as Cat and another one is Dog\n",
    "\n",
    "# First we have to loop through both the folders and conver all the images into arrays \n",
    "\n",
    "def prepare_Data(Dir,image_size):\n",
    "    data=[]                                                  # Creating empty list to store data\n",
    "    \n",
    "    categories=os.listdir(Dir)                               # Storing lables(folder names) in a list\n",
    "    \n",
    "    for category in categories:                              # Looping through each and every folder\n",
    "        \n",
    "        path=os.path.join(Dir,category)                      # joining current folder name with the path\n",
    "        \n",
    "        target=categories.index(category)                    # Storing the particular lable as target ['cat','Dog']\n",
    "        \n",
    "        for img in os.listdir(path):                         # Looping through the images \n",
    "            \n",
    "            image=cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE) # Reading the images\n",
    "            try:\n",
    "                image1=cv2.resize(image,(image_size,image_size)) # Images might be in different dimensions so resize it to same dimension\n",
    "                \n",
    "                data.append([image1,target])               # Appending resized image and its lable\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    np.random.shuffle(data)                                # Shuffling the data to avoid [\"cat\",\"cat\",\"cat\",\"dog\",\"dog\",\"gog\"]\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for feature , lable in data:\n",
    "        X.append(feature)                                  # Spliting X,y \n",
    "        y.append(lable)\n",
    "    return X,y,categories\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Call the function to create dataset\n",
    "\n",
    "X,y,data=image2array(Location,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now our data is ready but it is a list we have to convert it into array\n",
    "X=np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Shape of X is 3 dim but we need to convert it into 4 dim (-1,50,50,1) [No_sam,height,width,Channels]\n",
    "X=np.array(X).reshape(-1,50,50,1)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each and every time we can not prepare data so it is safe to save our data somewhere\n",
    "\n",
    "# for that i am going to use pickle\n",
    "\n",
    "# Saving the data into pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_in=open('cat_dog_X.pickle','wb')\n",
    "pickle.dump(X,data_in)\n",
    "data_in.close()\n",
    "data_y=open('cat_dog_y.pickle','wb')\n",
    "pickle.dump(y,data_y)\n",
    "data_y.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load our data from pickle\n",
    "\n",
    "# To open data from pickle\n",
    "x=open('cat_dog_X.pickle','rb')\n",
    "X=pickle.load(x)\n",
    "y=open('cat_dog_y.pickle','rb')\n",
    "y=pickle.load(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before feeding into model we have to normalize the data to get better performance\n",
    "\n",
    "# for that we need to first spilt the data\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y)\n",
    "\n",
    "X_train=X_train/255\n",
    "\n",
    "X_test=X_test/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hoty(y):\n",
    "    z=np.zeros((len(y),len(np.unique(y))),dtype=int)\n",
    "    for i in range(len(y)):\n",
    "        z[i,y[i]]=1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use this classifier for multiclass then you need have one-hot encoded y\n",
    "\n",
    "# y = one_hot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everuthing is ready now let's build the CNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First Convolution layer\n",
    "model.add(Conv2D(filters=65,kernel_size=4,input_shape=X.shape[1:],strides=1,padding='same',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=3))\n",
    "\n",
    "# Second Convolution layer\n",
    "model.add(Conv2D(filters=80,kernel_size=4,input_shape=X.shape[1:],strides=1,padding='same',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=3))\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully Connected layer1\n",
    "\n",
    "model.add(Dense(290,kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Fully Connected layer12\n",
    "\n",
    "model.add(Dense(70,kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Fully Connected layer3\n",
    "\n",
    "model.add(Dense(170,kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Fully Connected layer4\n",
    "\n",
    "model.add(Dense(50,kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Fully Connected layer5\n",
    "\n",
    "model.add(Dense(130,kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "# Output Layer\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Compling the model\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a binaer Classifier if you want to use it for multiclass classification then use softmax in output layer a\n",
    "# Also use one hot encoder to convert y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=100, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after running this model for 100 epoch on Gogle colab i got 88 percent accuracy on test data \n",
    "# if we tune even more we may get even better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Model works well but not giving good accuracy. for that we have to tune our model\n",
    "# I am going to use Keras Tuner\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "def build_function(hp):  # hp is the hyperparameter\n",
    "    \n",
    "    # first layer will be unique because it consist input shape so we are defining it outside the loop'\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=hp.Int('inupt_layer',10,100,5),kernel_size=4,input_shape=X_train.shape[1:],strides=1,\n",
    "                     padding='same',kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size =4))\n",
    "    \n",
    "    # Now it's time to decide how many con & Dense layer we need for good performance and \n",
    "    \n",
    "    # for that we are going to use just a for loop\n",
    "    \n",
    "    for i in range(hp.Int(\"conv_layers\",1,3)):\n",
    "        \n",
    "        # Here we need to specify the convolution layer without input size\n",
    "        # It will randomly pic no of layer from 1 to 5\n",
    "        \n",
    "        model.add(Conv2D(filters=hp.Int(f'hidden_conv_neurons{i}_',10,100,5),kernel_size=3,\n",
    "              strides=1,padding='same',kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size =4))\n",
    "    \n",
    "    # after conolution layer we need to flatten the data so let's add one Flatten layer\n",
    "    \n",
    "    model.add(Flatten())  # note it is outside the loop\n",
    "    \n",
    "    # here we have to use one more for loop for Dense layer\n",
    "    for j in range(hp.Int('Dense_layer',1,10)):\n",
    "        # It will randomly pic no of layer from 1 to 10\n",
    "        model.add(Dense(units=hp.Int(f\"Dense_{j}_layer\",50,300,20)))\n",
    "        \n",
    "        model.add(Dropout(0.2))             # To avaoid overfitting we are adding dropout it is a regularization technique\n",
    "        \n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    # Finally we need to add output layer\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid')) # This is a binary classification so we are using sigmoid\n",
    "    \n",
    "    # Now we need to compile the model\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = f\"{int(time.time())}\"\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_function,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,              \n",
    "    executions_per_trial=3,    \n",
    "    directory=LOG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train,y_train,verbose=2,\n",
    "             epochs=20,\n",
    "             batch_size=35,\n",
    "             validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
